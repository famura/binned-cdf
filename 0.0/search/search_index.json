{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Index","text":""},{"location":"#binned-cdf","title":"binned-cdf","text":"<p> <p>A PyTorch-based distribution parametrized by the logits of CDF bins</p>"},{"location":"#background","title":"Background","text":"<p>The Cumulative Distribution Function (CDF) is a fundamental concept in probability theory and statistics that describes the probability that a random variable \\(X\\) takes on a value less than or equal to a given threshold \\(x\\). Formally, the CDF is defined as \\(F(x) = P(X \\leq x)\\), where \\(F(x)\\) ranges from 0 to 1 as \\(x\\) varies from negative to positive infinity. The CDF provides a complete characterization of the probability distribution of a random variable: for continuous distributions, it is the integral of the probability density function (PDF), while for discrete distributions, it is the sum of probabilities up to and including \\(x\\). Key properties of any CDF are the monotonicity and the boundary conditions \\(\\lim_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim_{x \\to \\infty} F(x) = 1\\). CDFs are particularly useful for computing probabilities of intervals, quantiles, and for statistical inference.</p>"},{"location":"#application-to-machine-learning","title":"Application to Machine Learning","text":"<p>This repository uses the CDF to model and learn flexible probability distributions in machine learning tasks. By parameterizing the CDF with binned logits, it enables differentiable training and efficient sampling, making it suitable for uncertainty estimation, probabilistic prediction, and distributional modeling in neural networks.</p>"},{"location":"#implementation","title":"Implementation","text":"<p>The <code>BinnedLogitCDF</code> class inherits directly from <code>torch.distributions.Distribution</code>, implementing all necessary methods plus some convenience functions. It supports multi-dimensional batch shapes and CUDA devices. The bins can be initialized linearly or log-spaced.</p> <p><code>torch&gt;=2.7</code> it the only non-dev dependency of this repo.</p> <p> Please have a look at the documentation to get started.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v004-2025-11-17","title":"v0.0.4 - 2025-11-17","text":"<p>Compare with v0.0.3</p>"},{"location":"changelog/#v003-2025-11-17","title":"v0.0.3 - 2025-11-17","text":"<p>Compare with v0.0.2</p>"},{"location":"changelog/#v002-2025-11-17","title":"v0.0.2 - 2025-11-17","text":"<p>Compare with v0.0.1</p>"},{"location":"changelog/#v001-2025-11-17","title":"v0.0.1 - 2025-11-17","text":"<p>Compare with first commit</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>You can <code>pip install</code> different versions of this project as usual:</p> <pre><code>pip install binned-cdf\n</code></pre>"},{"location":"installation/#any-version-from-github","title":"Any Version from GitHub","text":"<p>You can install any version directly from GitHub:</p> <pre><code>GIT_REF=main # or some other ref, e.g. a branch or tag\npip install git+https://github.com/famura/binned-cdf@${GIT_REF}\n</code></pre>"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#this-project","title":"This Project","text":"<pre><code>    &lt;a href=\"https://github.com/famura/binned-cdf\"&gt;binned-cdf&lt;/a&gt; \u00a9 2025 by\n    &lt;a href=\"https://github.com/famura\"&gt;Fabio Muratore&lt;/a&gt; is licensed under\n    &lt;a href=\"https://creativecommons.org/licenses/by/4.0/\"&gt;CC BY 4.0&lt;/a&gt;\n    &lt;img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg\" alt=\"\" style=\"max-width: 1em;max-height:1em;margin-left: .2em;\"&gt;\n    &lt;img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg\" alt=\"\" style=\"max-width: 1em;max-height:1em;margin-left: .2em;\"&gt;\n</code></pre>"},{"location":"licenses/#third-party-libraries","title":"Third-Party Libraries","text":""},{"location":"licenses/#license-summary","title":"License Summary","text":"Count License 1 Academic Free License (AFL); MIT License 1 Apache 7 Apache Software License 4 Apache Software License; BSD License 1 Apache-2.0 22 BSD License 1 BSD-2-Clause 7 BSD-3-Clause 1 GNU Lesser General Public License v3 (LGPLv3) 5 ISC 1 ISC License (ISCL) 22 MIT 27 MIT License 1 MIT-CMU 2 MPL-2.0 2 Mozilla Public License 2.0 (MPL 2.0) 1 NVIDIA Proprietary Software 14 Other/Proprietary License 1 PSF-2.0 4 Python Software Foundation License 1 Unlicense"},{"location":"licenses/#details","title":"Details","text":"Name Version License Author URL Description GitPython 3.1.45 BSD-3-Clause Sebastian Thiel, Michael Trier https://github.com/gitpython-developers/GitPython GitPython is a Python library used to interact with Git repositories Jinja2 3.1.6 BSD License UNKNOWN https://github.com/pallets/jinja/ A very fast and expressive template engine. Markdown 3.10 BSD-3-Clause Manfred Stienstra, Yuri Takhteyev https://Python-Markdown.github.io/ Python implementation of John Gruber's Markdown. MarkupSafe 3.0.3 BSD-3-Clause UNKNOWN https://github.com/pallets/markupsafe/ Safely add untrusted strings to HTML/XML markup. PyYAML 6.0.3 MIT License Kirill Simonov https://pyyaml.org/ YAML parser and emitter for Python Pygments 2.19.2 BSD License Georg Brandl georg@python.org https://pygments.org Pygments is a syntax highlighting package written in Python. autocommand 2.2.2 GNU Lesser General Public License v3 (LGPLv3) Nathan West https://github.com/Lucretiel/autocommand A library to create a command-line program from a function babel 2.17.0 BSD License Armin Ronacher https://babel.pocoo.org/ Internationalization utilities backports.tarfile 1.2.0 MIT License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/backports.tarfile Backport of CPython tarfile module backrefs 6.1 MIT Isaac Muse Isaac.Muse@gmail.com https://github.com/facelessuser/backrefs A wrapper around re and regex that adds additional back references. certifi 2025.11.12 Mozilla Public License 2.0 (MPL 2.0) Kenneth Reitz https://github.com/certifi/python-certifi Python package for providing Mozilla's CA Bundle. cfgv 3.4.0 MIT License Anthony Sottile https://github.com/asottile/cfgv Validate configuration and produce human readable error messages. charset-normalizer 3.4.4 MIT \"Ahmed R. TAHRI\" tahri.ahmed@proton.me https://github.com/jawah/charset_normalizer/blob/master/CHANGELOG.md The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. click 8.3.1 BSD-3-Clause UNKNOWN https://github.com/pallets/click/ Composable command line interface toolkit colorama 0.4.6 BSD License Jonathan Hartley tartley@tartley.com https://github.com/tartley/colorama Cross-platform colored terminal text. contourpy 1.3.3 BSD License Ian Thomas ianthomas23@gmail.com https://github.com/contourpy/contourpy Python library for calculating contours of 2D quadrilateral grids coverage 7.11.3 Apache-2.0 Ned Batchelder and 244 others https://github.com/coveragepy/coveragepy Code coverage measurement for Python csscompressor 0.9.5 BSD License Yury Selivanov http://github.com/sprymix/csscompressor A python port of YUI CSS Compressor cycler 0.12.1 BSD License Thomas A Caswell matplotlib-users@python.org https://matplotlib.org/cycler/ Composable style cycles defusedxml 0.7.1 Python Software Foundation License Christian Heimes https://github.com/tiran/defusedxml XML bomb protection for Python stdlib modules distlib 0.4.0 Python Software Foundation License Vinay Sajip https://github.com/pypa/distlib Distribution utilities execnet 2.1.2 MIT holger krekel and others https://execnet.readthedocs.io/en/latest/ execnet: rapid multi-Python deployment filelock 3.20.0 Unlicense UNKNOWN https://github.com/tox-dev/py-filelock A platform independent file lock. fonttools 4.60.1 MIT Just van Rossum http://github.com/fonttools/fonttools Tools to manipulate font files fsspec 2025.10.0 BSD-3-Clause UNKNOWN https://github.com/fsspec/filesystem_spec File-system specification genbadge 1.1.2 BSD License Sylvain MARIE sylvain.marie@se.com https://github.com/smarie/python-genbadge Generate badges for tools that do not provide one. ghp-import 2.1.0 Apache Software License Paul Joseph Davis https://github.com/c-w/ghp-import Copy your docs directly to the gh-pages branch. git-changelog 2.6.3 ISC =?utf-8?q?Timoth=C3=A9e_Mazzucotelli?= dev@pawamoy.fr https://pawamoy.github.io/git-changelog Automatic Changelog generator using Jinja2 templates. gitdb 4.0.12 BSD License Sebastian Thiel https://github.com/gitpython-developers/gitdb Git Object Database griffe 1.15.0 ISC =?utf-8?q?Timoth=C3=A9e_Mazzucotelli?= dev@pawamoy.fr https://mkdocstrings.github.io/griffe Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. hjson 3.1.0 Academic Free License (AFL); MIT License Christian Zangl http://github.com/hjson/hjson-py Hjson, a user interface for JSON. htmlmin2 0.1.13 BSD License Dave Mankoff https://htmlmin.readthedocs.io/en/latest/ An HTML Minifier identify 2.6.15 MIT Chris Kuehl https://github.com/pre-commit/identify File identification library for Python idna 3.11 BSD-3-Clause Kim Davies kim+pypi@gumleaf.org https://github.com/kjd/idna Internationalized Domain Names in Applications (IDNA) importlib_metadata 8.0.0 Apache Software License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/python/importlib_metadata Read metadata from Python packages importlib_metadata 8.7.0 Apache Software License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/python/importlib_metadata Read metadata from Python packages importlib_resources 6.5.2 Apache Software License Barry Warsaw barry@python.org https://github.com/python/importlib_resources Read resources from Python packages inflect 7.3.1 MIT License Paul Dyson pwdyson@yahoo.com https://github.com/jaraco/inflect Correctly generate plurals, singular nouns, ordinals, indefinite articles iniconfig 2.3.0 MIT Ronny Pfannschmidt opensource@ronnypfannschmidt.de, Holger Krekel holger.krekel@gmail.com https://github.com/pytest-dev/iniconfig brain-dead simple config-ini parsing jaraco.collections 5.1.0 MIT License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/jaraco.collections Collection objects similar to those in stdlib by jaraco jaraco.context 5.3.0 MIT License Jason R. Coombs https://github.com/jaraco/jaraco.context Useful decorators and context managers jaraco.functools 4.0.1 MIT License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/jaraco.functools Functools like those found in stdlib jaraco.text 3.12.1 MIT License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/jaraco.text Module for text manipulation jsmin 3.0.1 MIT License Dave St.Germain https://github.com/tikitu/jsmin/ JavaScript minifier. kiwisolver 1.4.9 BSD License The Nucleic Development Team sccolbert@gmail.com https://github.com/nucleic/kiwi A fast implementation of the Cassowary constraint solver markdown-it-py 4.0.0 MIT License Chris Sewell chrisj_sewell@hotmail.com https://github.com/executablebooks/markdown-it-py Python port of markdown-it. Markdown parsing, done right! matplotlib 3.10.7 Python Software Foundation License John D. Hunter, Michael Droettboom https://matplotlib.org Python plotting package mdurl 0.1.2 MIT License Taneli Hukkinen hukkin@users.noreply.github.com https://github.com/executablebooks/mdurl Markdown URL utilities mergedeep 1.3.4 MIT License Travis Clarke https://github.com/clarketm/mergedeep A deep merge function for \ud83d\udc0d. mike 2.1.3 BSD License Jim Porter https://github.com/jimporter/mike Manage multiple versions of your MkDocs-powered documentation mkdocs 1.6.1 BSD-2-Clause Tom Christie tom@tomchristie.com https://github.com/mkdocs/mkdocs Project documentation with Markdown. mkdocs-autorefs 1.4.3 ISC Oleh Prypin oleh@pryp.in, =?utf-8?q?Timoth=C3=A9e_Mazzucotelli?= dev@pawamoy.fr https://mkdocstrings.github.io/autorefs Automatically link across pages in MkDocs. mkdocs-get-deps 0.2.0 MIT Oleh Prypin oleh@pryp.in https://github.com/mkdocs/get-deps MkDocs extension that lists all dependencies according to a mkdocs.yml file mkdocs-git-revision-date-localized-plugin 1.5.0 MIT License Tim Vink vinktim@gmail.com https://github.com/timvink/mkdocs-git-revision-date-localized-plugin Mkdocs plugin that enables displaying the localized date of the last git modification of a markdown file. mkdocs-macros-plugin 1.5.0 MIT License Laurent Franceschetti https://github.com/fralau/mkdocs_macros_plugin Unleash the power of MkDocs with macros and variables mkdocs-material 9.7.0 MIT Martin Donath martin.donath@squidfunk.com https://github.com/squidfunk/mkdocs-material Documentation that simply works mkdocs-material-extensions 1.3.1 MIT Isaac Muse Isaac.Muse@gmail.com https://github.com/facelessuser/mkdocs-material-extensions Extension pack for Python Markdown and MkDocs Material. mkdocs-minify-plugin 0.8.0 MIT License Byrne Reese, Lars Wilhelmer https://github.com/byrnereese/mkdocs-minify-plugin An MkDocs plugin to minify HTML, JS or CSS files prior to being written to disk mkdocs-typer 0.0.3 Apache Bruce Szalwinski https://github.com/bruce-szalwinski/mkdocs-typer An MkDocs extension to generate documentation for Typer command line applications mkdocstrings 0.30.1 ISC =?utf-8?q?Timoth=C3=A9e_Mazzucotelli?= dev@pawamoy.fr https://mkdocstrings.github.io Automatic documentation from sources, for MkDocs. mkdocstrings-python 1.19.0 ISC =?utf-8?q?Timoth=C3=A9e_Mazzucotelli?= dev@pawamoy.fr https://mkdocstrings.github.io/python A Python handler for mkdocstrings. more-itertools 10.3.0 MIT License Erik Rose erikrose@grinchcentral.com https://github.com/more-itertools/more-itertools More routines for operating on iterables, beyond itertools mpmath 1.3.0 BSD License Fredrik Johansson http://mpmath.org/ Python library for arbitrary-precision floating-point arithmetic networkx 3.5 BSD License Aric Hagberg hagberg@lanl.gov https://networkx.org/ Python package for creating and manipulating graphs and networks nodeenv 1.9.1 BSD License Eugene Kalinin https://github.com/ekalinin/nodeenv Node.js virtual environment builder numpy 2.3.5 BSD License Travis E. Oliphant et al. https://numpy.org Fundamental package for array computing in Python nvidia-cublas-cu12 12.8.4.1 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUBLAS native runtime libraries nvidia-cuda-cupti-cu12 12.8.90 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUDA profiling tools runtime libs. nvidia-cuda-nvrtc-cu12 12.8.93 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone NVRTC native runtime libraries nvidia-cuda-runtime-cu12 12.8.90 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUDA Runtime native Libraries nvidia-cudnn-cu12 9.10.2.21 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone cuDNN runtime libraries nvidia-cufft-cu12 11.3.3.83 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUFFT native runtime libraries nvidia-cufile-cu12 1.13.1.3 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone cuFile GPUDirect libraries nvidia-curand-cu12 10.3.9.90 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CURAND native runtime libraries nvidia-cusolver-cu12 11.7.3.90 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUDA solver native runtime libraries nvidia-cusparse-cu12 12.5.8.93 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUSPARSE native runtime libraries nvidia-cusparselt-cu12 0.7.1 NVIDIA Proprietary Software NVIDIA Corporation https://developer.nvidia.com/cusparselt NVIDIA cuSPARSELt nvidia-nccl-cu12 2.27.5 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone NVIDIA Collective Communication Library (NCCL) Runtime nvidia-nvjitlink-cu12 12.8.93 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone Nvidia JIT LTO Library nvidia-nvshmem-cu12 3.3.20 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone NVSHMEM creates a global address space that provides efficient and scalable communication for NVIDIA GPU clusters. nvidia-nvtx-cu12 12.8.90 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone NVIDIA Tools Extension packaging 24.2 Apache Software License; BSD License Donald Stufft donald@stufft.io https://github.com/pypa/packaging Core utilities for Python packages packaging 25.0 Apache Software License; BSD License Donald Stufft donald@stufft.io https://github.com/pypa/packaging Core utilities for Python packages paginate 0.5.7 MIT License Christoph Haas https://github.com/Signum/paginate Divides large result sets into pages for easier browsing pandas 2.3.3 BSD License The Pandas Development Team pandas-dev@python.org https://pandas.pydata.org Powerful data structures for data analysis, time series, and statistics pathspec 0.12.1 Mozilla Public License 2.0 (MPL 2.0) \"Caleb P. Burns\" cpburnz@gmail.com UNKNOWN Utility library for gitignore style pattern matching of file paths. pillow 12.0.0 MIT-CMU \"Jeffrey A. Clark\" aclark@aclark.net https://python-pillow.github.io Python Imaging Library (fork) platformdirs 4.2.2 MIT UNKNOWN https://github.com/platformdirs/platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. platformdirs 4.5.0 MIT UNKNOWN https://github.com/tox-dev/platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. pluggy 1.6.0 MIT License Holger Krekel holger@merlinux.eu UNKNOWN plugin and hook calling mechanisms for python pre_commit 4.4.0 MIT Anthony Sottile https://github.com/pre-commit/pre-commit A framework for managing and maintaining multi-language pre-commit hooks. pymdown-extensions 10.17.1 MIT Isaac Muse Isaac.Muse@gmail.com https://github.com/facelessuser/pymdown-extensions Extension pack for Python Markdown. pyparsing 3.2.5 MIT Paul McGuire ptmcg.gm+pyparsing@gmail.com https://github.com/pyparsing/pyparsing/ pyparsing - Classes and methods to define and execute parsing grammars pytest 9.0.1 MIT Holger Krekel, Bruno Oliveira, Ronny Pfannschmidt, Floris Bruynooghe, Brianna Laugher, Florian Bruhin, Others (See AUTHORS) https://docs.pytest.org/en/latest/ pytest: simple powerful testing with Python pytest-cov 7.0.0 MIT Marc Schlaich marc.schlaich@gmail.com https://pytest-cov.readthedocs.io/en/latest/changelog.html Pytest plugin for measuring coverage. pytest-html 4.1.1 MPL-2.0 Dave Hunt dhunt@mozilla.com, Jim Brannlund jimbrannlund@fastmail.com https://github.com/pytest-dev/pytest-html pytest plugin for generating HTML reports pytest-metadata 3.1.1 MPL-2.0 Dave Hunt dhunt@mozilla.com, Jim Brannlund jimbrannlund@fastmail.com https://github.com/pytest-dev/pytest-metadata pytest plugin for test session metadata pytest-xdist 3.8.0 MIT holger krekel and contributors pytest-dev@python.org, holger@merlinux.eu https://github.com/pytest-dev/pytest-xdist pytest xdist plugin for distributed testing, most importantly across multiple CPUs python-dateutil 2.9.0.post0 Apache Software License; BSD License Gustavo Niemeyer https://github.com/dateutil/dateutil Extensions to the standard Python datetime module pytz 2025.2 MIT License Stuart Bishop http://pythonhosted.org/pytz World timezone definitions, modern and historical pyyaml_env_tag 1.1 MIT Waylan Limberg waylan.limberg@icloud.com https://github.com/waylan/pyyaml-env-tag A custom YAML tag for referencing environment variables in YAML files. requests 2.32.5 Apache Software License Kenneth Reitz https://requests.readthedocs.io Python HTTP for Humans. rich 14.2.0 MIT License Will McGugan https://github.com/Textualize/rich Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal scipy 1.16.3 BSD License UNKNOWN https://scipy.org/ Fundamental algorithms for scientific computing in Python seaborn 0.13.2 BSD License Michael Waskom mwaskom@gmail.com https://github.com/mwaskom/seaborn Statistical data visualization semver 3.0.4 BSD License Kostiantyn Rybnikov k-bx@k-bx.com, Tom Schraitle toms@suse.de https://python-semver.readthedocs.io/en/latest/changelog.html Python helper for Semantic Versioning (https://semver.org) shellingham 1.5.4 ISC License (ISCL) Tzu-ping Chung https://github.com/sarugaku/shellingham Tool to Detect Surrounding Shell six 1.17.0 MIT License Benjamin Peterson https://github.com/benjaminp/six Python 2 and 3 compatibility utilities smmap 5.0.2 BSD License Sebastian Thiel https://github.com/gitpython-developers/smmap A pure Python implementation of a sliding window memory map manager super_collections 0.6.2 MIT License Laurent Franceschetti https://github.com/fralau/super-collections file: README.md sympy 1.14.0 BSD License SymPy development team https://sympy.org Computer algebra system (CAS) in Python termcolor 3.2.0 MIT Konstantin Lepa konstantin.lepa@gmail.com https://github.com/termcolor/termcolor ANSI color formatting for output in terminal tomli 2.0.1 MIT License Taneli Hukkinen hukkin@users.noreply.github.com https://github.com/hukkin/tomli A lil' TOML parser torch 2.9.1 BSD-3-Clause PyTorch Team packages@pytorch.org https://pytorch.org Tensors and Dynamic neural networks in Python with strong GPU acceleration triton 3.5.1 MIT License Philippe Tillet https://github.com/triton-lang/triton/ A language and compiler for custom Deep Learning operations typeguard 4.3.0 MIT License Alex Gr\u00f6nholm alex.gronholm@nextday.fi UNKNOWN Run-time type checker for Python typer 0.20.0 MIT License =?utf-8?q?Sebasti=C3=A1n_Ram=C3=ADrez?= tiangolo@gmail.com https://github.com/fastapi/typer Typer, build great CLIs. Easy to code. Based on Python type hints. typing_extensions 4.12.2 Python Software Foundation License \"Guido van Rossum, Jukka Lehtosalo, \u0141ukasz Langa, Michael Lee\" levkivskyi@gmail.com https://github.com/python/typing_extensions Backported and Experimental Type Hints for Python 3.8+ typing_extensions 4.15.0 PSF-2.0 \"Guido van Rossum, Jukka Lehtosalo, \u0141ukasz Langa, Michael Lee\" levkivskyi@gmail.com https://github.com/python/typing_extensions Backported and Experimental Type Hints for Python 3.9+ tzdata 2025.2 Apache Software License Python Software Foundation https://github.com/python/tzdata Provider of IANA time zone data urllib3 2.5.0 MIT Andrey Petrov andrey.petrov@shazow.net https://github.com/urllib3/urllib3/blob/main/CHANGES.rst HTTP library with thread-safe connection pooling, file post, and more. verspec 0.1.0 Apache Software License; BSD License Jim Porter https://github.com/jimporter/verspec Flexible version handling virtualenv 20.35.4 MIT UNKNOWN https://github.com/pypa/virtualenv Virtual Python Environment builder watchdog 6.0.0 Apache Software License Micka\u00ebl Schoentgen https://github.com/gorakhargosh/watchdog Filesystem events monitoring zipp 3.19.2 MIT License \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/zipp Backport of pathlib-compatible object wrapper for zip files zipp 3.23.0 MIT \"Jason R. Coombs\" jaraco@jaraco.com https://github.com/jaraco/zipp Backport of pathlib-compatible object wrapper for zip files"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#binned_cdf.binned_logit_cdf","title":"<code>binned_cdf.binned_logit_cdf</code>","text":""},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF","title":"<code>BinnedLogitCDF</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A histogram-based probability distribution parameterized by a bins for the CDF.</p> <p>Each bin contributes a step function to the CDF when active. The activation of each bin is determined by applying a sigmoid to the corresponding logit. The distribution is defined over the interval [bound_low, bound_up] with either linear or logarithmic bin spacing.</p> <p>Note</p> <p>This distribution is differentiable with respect to the logits, i.e., the arguments of <code>__init__</code>, but not through the inputs of the <code>prob</code> or <code>cfg</code> method.</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>class BinnedLogitCDF(Distribution):\n    \"\"\"A histogram-based probability distribution parameterized by a bins for the CDF.\n\n    Each bin contributes a step function to the CDF when active.\n    The activation of each bin is determined by applying a sigmoid to the corresponding logit.\n    The distribution is defined over the interval [bound_low, bound_up] with either linear or logarithmic bin spacing.\n\n    Note:\n        This distribution is differentiable with respect to the logits, i.e., the arguments of `__init__`, but\n        not through the inputs of the `prob` or `cfg` method.\n    \"\"\"\n\n    def __init__(\n        self,\n        logits: torch.Tensor,\n        bound_low: float = -1e3,\n        bound_up: float = 1e3,\n        log_spacing: bool = False,\n        validate_args: bool | None = None,\n    ) -&gt; None:\n        \"\"\"Initializer.\n\n        Args:\n            logits: Raw logits for bin probabilities (before sigmoid), of shape (*batch_shape, num_bins)\n            bound_low: Lower bound of the distribution support, needs to be finite.\n            bound_up: Upper bound of the distribution support, needs to be finite.\n            log_spacing: Whether logarithmic (base = 2) spacing for the bins or linear spacing should be used.\n            validate_args: Whether to validate arguments. Carried over to keep the interface with the base class.\n        \"\"\"\n        self.logits = logits\n        self.bound_low = bound_low\n        self.bound_up = bound_up\n        self.log_spacing = log_spacing\n\n        # Create bin structure (same for all batch dimensions).\n        self.bin_edges, self.bin_centers, self.bin_widths = self._create_bins(\n            num_bins=logits.shape[-1],\n            bound_low=bound_low,\n            bound_up=bound_up,\n            log_spacing=log_spacing,\n            device=logits.device,\n            dtype=logits.dtype,\n        )\n\n        super().__init__(batch_shape=logits.shape[:-1], event_shape=torch.Size([]), validate_args=validate_args)\n\n    @classmethod\n    def _create_bins(\n        cls,\n        num_bins: int,\n        bound_low: float,\n        bound_up: float,\n        log_spacing: bool,\n        device: torch.device,\n        dtype: torch.dtype,\n        log_min_positive_edge: float = 1e-6,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Create bin edges with symmetric log spacing around zero.\n\n        Args:\n            num_bins: Number of bins to create.\n            bound_low: Lower bound of the distribution support.\n            bound_up: Upper bound of the distribution support.\n            log_spacing: Whether to use logarithmic spacing.\n            device: Device for the tensors.\n            dtype: Data type for the tensors.\n            log_min_positive_edge: Minimum positive edge when using log spacing. The log2-value of this argument\n                will be passed to torch.logspace. Too small values, approx below 1e-9, will result in poor bin spacing.\n\n        Returns:\n            Tuple of (bin_edges, bin_centers, bin_widths).\n\n        Layout:\n            - 1 edge at 0\n            - num_bins//2 - 1 edges from 0 to bound_up (log spaced)\n            - num_bins//2 - 1 edges from 0 to -bound_low (log spaced, mirrored)\n            - 2 boundary edges at \u00b1bounds\n\n        Total: num_bins + 1 edges creating num_bins bins\n        \"\"\"\n        if log_spacing:\n            if not math.isclose(-bound_low, bound_up):\n                raise ValueError(\"log_spacing requires symmetric bounds: -bound_low == bound_up\")\n            if bound_up &lt;= 0:\n                raise ValueError(\"log_spacing requires bound_up &gt; 0\")\n            if num_bins % 2 != 0:\n                raise ValueError(\"log_spacing requires even number of bins\")\n\n            half_bins = num_bins // 2\n\n            # Create positive side: 0, internal edges, bound_up.\n            if half_bins == 1:\n                # Special case: only boundary edges.\n                positive_edges = torch.tensor([0.0, bound_up])\n            else:\n                # Create half_bins - 1 internal edges between 0 and bound_up.\n                internal_positive = torch.logspace(\n                    start=math.log2(log_min_positive_edge),\n                    end=math.log2(bound_up),\n                    steps=half_bins,\n                    base=2,\n                )\n                positive_edges = torch.cat([internal_positive[:-1], torch.tensor([bound_up])])\n\n            # Mirror for the negative side (excluding 0).\n            negative_edges = -positive_edges.flip(0)\n\n            # Combine to [negative_boundary, negative_internal, 0, positive_internal, positive_boundary].\n            bin_edges = torch.cat([negative_edges, torch.tensor([0.0]), positive_edges])\n\n        else:\n            # Linear spacing.\n            bin_edges = torch.linspace(start=bound_low, end=bound_up, steps=num_bins + 1)\n\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) * 0.5\n        bin_widths = bin_edges[1:] - bin_edges[:-1]\n\n        # Move to specified device and dtype.\n        bin_edges = bin_edges.to(device=device, dtype=dtype)\n        bin_centers = bin_centers.to(device=device, dtype=dtype)\n        bin_widths = bin_widths.to(device=device, dtype=dtype)\n\n        return bin_edges, bin_centers, bin_widths\n\n    @property\n    def num_bins(self) -&gt; int:\n        \"\"\"Number of bins making up the BinnedLogitCDF.\"\"\"\n        return self.logits.shape[-1]\n\n    @property\n    def num_edges(self) -&gt; int:\n        \"\"\"Number of bins edges of the BinnedLogitCDF.\"\"\"\n        return self.bin_edges.shape[0]\n\n    @property\n    def probs(self) -&gt; torch.Tensor:\n        \"\"\"Get normalized probabilities for each bin, of shape (*batch_shape, num_bins).\"\"\"\n        raw_probs = torch.sigmoid(self.logits)  # shape: (*batch_shape, num_bins)\n        return raw_probs / raw_probs.sum(dim=-1, keepdim=True)\n\n    @property\n    def mean(self) -&gt; torch.Tensor:\n        \"\"\"Compute mean of the distribution, i.e., the weighted average of bin centers, of shape (*batch_size,).\"\"\"\n        bin_probs = self.probs\n        weighted_centers = bin_probs * self.bin_centers  # shape: (*batch_shape, num_bins)\n        return torch.sum(weighted_centers, dim=-1)\n\n    @property\n    def variance(self) -&gt; torch.Tensor:\n        \"\"\"Compute variance of the distribution, of shape (*batch_shape,).\"\"\"\n        bin_probs = self.probs\n\n        # E[X^2] = weighted squared bin centers.\n        weighted_centers_sq = bin_probs * (self.bin_centers**2)  # shape: (*batch_shape, num_bins)\n        second_moment = torch.sum(weighted_centers_sq, dim=-1)  # shape: (*batch_shape,)\n\n        # Var = E[X^2] - E[X]^2\n        return second_moment - self.mean**2\n\n    @property\n    def support(self) -&gt; constraints.Constraint:\n        \"\"\"Support of this distribution. Needs to be limitited to keep the number of bins manageable.\"\"\"\n        return constraints.interval(self.bound_low, self.bound_up)\n\n    @property\n    def arg_constraints(self) -&gt; dict[str, constraints.Constraint]:\n        \"\"\"Constraints that should be satisfied by each argument of this distribution. None for this class.\"\"\"\n        return {}\n\n    def expand(\n        self, batch_shape: torch.Size | list[int] | tuple[int, ...], _instance: Distribution | None = None\n    ) -&gt; \"BinnedLogitCDF\":\n        \"\"\"Expand distribution to new batch shape. This creates a new instance.\"\"\"\n        expanded_logits = self.logits.expand((*torch.Size(batch_shape), self.num_bins))\n        return BinnedLogitCDF(\n            logits=expanded_logits,\n            bound_low=self.bound_low,\n            bound_up=self.bound_up,\n            log_spacing=self.log_spacing,\n            validate_args=self._validate_args,\n        )\n\n    def log_prob(self, value: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute log probability density at given values.\"\"\"\n        return torch.log(self.prob(value) + 1e-8)  # small epsilon for stability\n\n    def prob(self, value: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute probability density at given values.\n\n        Args:\n            value: Values at which to compute the PDF.\n                Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n        Returns:\n            PDF values corresponding to the input values.\n            Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n        \"\"\"\n        if self._validate_args:\n            self._validate_sample(value)\n\n        value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n        # Determine number of sample dimensions (dimensions before batch_shape).\n        num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n        # Prepend singleton dimensions for sample_shape to bin_edges, bin_widths, and probs.\n        # For all of them, the resulting shape will be: (*sample_shape, *batch_shape, num_bins)\n        bin_edges_left = self.bin_edges[..., :-1]  # shape: (*batch_shape, num_bins)\n        bin_edges_right = self.bin_edges[..., 1:]  # shape: (*batch_shape, num_bins)\n        bin_edges_left = bin_edges_left.view((1,) * num_sample_dims + bin_edges_left.shape)\n        bin_edges_right = bin_edges_right.view((1,) * num_sample_dims + bin_edges_right.shape)\n        bin_widths = self.bin_widths.view((1,) * num_sample_dims + self.bin_widths.shape)\n        probs = self.probs.view((1,) * num_sample_dims + self.probs.shape)\n\n        # Add bin dimension to value for broadcasting.\n        value_expanded = value.unsqueeze(-1)  # shape: (*sample_shape, *batch_shape, 1)\n\n        # Check which bin each value falls into. Result shape: (*sample_shape, *batch_shape, num_bins).\n        in_bin = ((value_expanded &gt;= bin_edges_left) &amp; (value_expanded &lt; bin_edges_right)).to(self.logits.dtype)\n\n        # Handle right edge case (include bound_up in last bin).\n        at_right_edge = torch.isclose(\n            value_expanded, torch.tensor(self.bound_up, dtype=self.logits.dtype, device=self.logits.device)\n        )\n        in_bin[..., -1] = torch.max(in_bin[..., -1], at_right_edge[..., -1])\n\n        # PDF = (probability mass / bin width) for the containing bin.\n        pdf_per_bin = probs / bin_widths  # shape: (*sample_shape, *batch_shape, num_bins)\n\n        # Sum over bins is the same as selecting the bin, as there is only one bin active per value.\n        return torch.sum(in_bin * pdf_per_bin, dim=-1)\n\n    def cdf(self, value: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute cumulative distribution function at given values.\n\n        Args:\n            value: Values at which to compute the CDF.\n                Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n        Returns:\n            CDF values in [0, 1] corresponding to the input values.\n            Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n        \"\"\"\n        if self._validate_args:\n            self._validate_sample(value)\n\n        value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n        # Determine number of sample dimensions (dimensions before batch_shape).\n        num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n        # Prepend singleton dimensions for sample_shape to bin_centers.\n        # bin_centers: (*batch_shape, num_bins) -&gt; (*sample_shape, *batch_shape, num_bins)\n        bin_centers_expanded = self.bin_centers.view((1,) * num_sample_dims + self.bin_centers.shape)\n\n        # Prepend singleton dimensions for sample_shape to probs.\n        # probs: (*batch_shape, num_bins) -&gt; (*sample_shape, *batch_shape, num_bins)\n        probs_expanded = self.probs.view((1,) * num_sample_dims + self.probs.shape)\n\n        # Add the bin dimension to the input which is used for comparing with the bin centers.\n        value_expanded = value.unsqueeze(-1)  # shape: (*sample_shape, *batch_shape, 1)\n\n        # Mask for bins with centers &lt;= value.\n        mask = bin_centers_expanded &lt;= value_expanded  # shape: (*sample_shape, *batch_shape, num_bins)\n\n        # Sum the bins for this value by their weighted \"activation\"=probability.\n        return torch.sum(mask * probs_expanded, dim=-1)\n\n    def icdf(self, value: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the inverse CDF, i.e., the quantile function, at the given values.\n\n        Args:\n            value: Values in [0, 1] at which to compute the inverse CDF.\n                Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n        Returns:\n            Quantiles in [bound_low, bound_up] corresponding to the input CDF values.\n            Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n        \"\"\"\n        if self._validate_args and not (value &gt;= 0).all() and (value &lt;= 1).all():\n            raise ValueError(\"icdf input must be in [0, 1]\")\n\n        value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n        # Compute CDF at bin edges. prepend zeros to the cumsum of probabilities as this is always the first edge.\n        cdf_edges = torch.cat(\n            [\n                torch.zeros(*self.batch_shape, 1, dtype=self.logits.dtype, device=self.logits.device),\n                torch.cumsum(self.probs, dim=-1),  # shape: (*batch_shape, num_bins)\n            ],\n            dim=-1,\n        )  # shape: (*batch_shape, num_bins + 1)\n\n        # Determine number of sample dimensions (dimensions before batch_shape).\n        num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n        # Prepend singleton dimensions for sample_shape to cdf_edges.\n        # cdf_edges: (*batch_shape, num_bins + 1) -&gt; (*sample_shape, *batch_shape, num_bins + 1)\n        cdf_edges = cdf_edges.view((1,) * num_sample_dims + cdf_edges.shape)\n\n        # Prepend singleton dimensions for  both sample_shape and batch_shape.\n        # bin_edges: (num_bins + 1,) -&gt; (*sample_shape, *batch_shape, num_bins + 1)\n        bin_edges_expanded = self.bin_edges.view(\n            (1,) * (num_sample_dims + len(self.batch_shape)) + self.bin_edges.shape\n        )\n\n        # Add bin dimension to value for comparison.\n        value_expanded = value.unsqueeze(-1)\n\n        # Find bins containing the value: left_cdf &lt;= value &lt; right_cdf.\n        bin_mask = (cdf_edges[..., :-1] &lt;= value_expanded) &amp; (value_expanded &lt; cdf_edges[..., 1:])\n        bin_mask = bin_mask.to(self.logits.dtype)\n\n        # Handle edge case where value \u2248 1.0 (use isclose with dtype-appropriate defaults).\n        value_is_one = torch.isclose(value_expanded, torch.ones_like(value_expanded))\n        bin_mask[..., -1] = torch.max(bin_mask[..., -1], value_is_one[..., 0])  # last bin could be selected already\n\n        # Selected the correct bin edges using the mask. Summing is essentially selecting here.\n        # Summing fast and differentiable.\n        cfd_value_bin_starts = torch.sum(bin_mask * cdf_edges[..., :-1], dim=-1)\n        cdf_value_bin_ends = torch.sum(bin_mask * cdf_edges[..., 1:], dim=-1)\n        bin_left_edges = torch.sum(bin_mask * bin_edges_expanded[..., :-1], dim=-1)\n        bin_right_edges = torch.sum(bin_mask * bin_edges_expanded[..., 1:], dim=-1)\n\n        # Avoid division by zero.\n        bin_width = cdf_value_bin_ends - cfd_value_bin_starts\n        safe_bin_width = torch.where(bin_width &gt; 1e-8, bin_width, torch.ones_like(bin_width))\n\n        # Linear interpolation within the bin.\n        alpha = (value - cfd_value_bin_starts) / safe_bin_width\n        quantiles = bin_left_edges + alpha * (bin_right_edges - bin_left_edges)\n\n        return quantiles\n\n    @torch.no_grad()\n    def sample(self, sample_shape: torch.Size | list[int] | tuple[int, ...] = _size) -&gt; torch.Tensor:\n        \"\"\"Sample from the distribution by passing uniformly random draws from [0, 1] thought the inverse CDF.\n\n        Args:\n            sample_shape: Shape of the samples to draw.\n\n        Returns:\n            Samples of shape [sample_shape + batch_shape, num_bins].\n        \"\"\"\n        shape = torch.Size(sample_shape) + self.batch_shape\n        uniform_samples = torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device)\n        return self.icdf(uniform_samples)\n\n    def entropy(self) -&gt; torch.Tensor:\n        r\"\"\"Compute differential entropy of the distribution.\n\n        Entropy H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log( p(x) )\n\n        Note:\n            Here, we are doing an approximation by treating each bin as a uniform distribution over its width.\n        \"\"\"\n        bin_probs = self.probs\n\n        # Get the PDF values at bin centers.\n        pdf_values = bin_probs / self.bin_widths  # shape: (*batch_shape, num_bins)\n\n        # Entropy \u2248 -\u2211 p_i * log(pdf_i) * bin_width_i.\n        log_pdf = torch.log(pdf_values + 1e-8)  # small epsilon for stability\n        entropy_per_bin = -bin_probs * log_pdf\n\n        # Sum over bins to get total entropy.\n        return torch.sum(entropy_per_bin, dim=-1)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the distribution.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(logits_shape: {self.logits.shape}, bound_low: {self.bound_low}, \"\n            f\"bound_up: {self.bound_up}, log_spacing: {self.log_spacing})\"\n        )\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.arg_constraints","title":"<code>arg_constraints</code>  <code>property</code>","text":"<p>Constraints that should be satisfied by each argument of this distribution. None for this class.</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Compute mean of the distribution, i.e., the weighted average of bin centers, of shape (*batch_size,).</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.num_bins","title":"<code>num_bins</code>  <code>property</code>","text":"<p>Number of bins making up the BinnedLogitCDF.</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.num_edges","title":"<code>num_edges</code>  <code>property</code>","text":"<p>Number of bins edges of the BinnedLogitCDF.</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.probs","title":"<code>probs</code>  <code>property</code>","text":"<p>Get normalized probabilities for each bin, of shape (*batch_shape, num_bins).</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.support","title":"<code>support</code>  <code>property</code>","text":"<p>Support of this distribution. Needs to be limitited to keep the number of bins manageable.</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.variance","title":"<code>variance</code>  <code>property</code>","text":"<p>Compute variance of the distribution, of shape (*batch_shape,).</p>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.__init__","title":"<code>__init__(logits, bound_low=-1000.0, bound_up=1000.0, log_spacing=False, validate_args=None)</code>","text":"<p>Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Raw logits for bin probabilities (before sigmoid), of shape (*batch_shape, num_bins)</p> required <code>bound_low</code> <code>float</code> <p>Lower bound of the distribution support, needs to be finite.</p> <code>-1000.0</code> <code>bound_up</code> <code>float</code> <p>Upper bound of the distribution support, needs to be finite.</p> <code>1000.0</code> <code>log_spacing</code> <code>bool</code> <p>Whether logarithmic (base = 2) spacing for the bins or linear spacing should be used.</p> <code>False</code> <code>validate_args</code> <code>bool | None</code> <p>Whether to validate arguments. Carried over to keep the interface with the base class.</p> <code>None</code> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def __init__(\n    self,\n    logits: torch.Tensor,\n    bound_low: float = -1e3,\n    bound_up: float = 1e3,\n    log_spacing: bool = False,\n    validate_args: bool | None = None,\n) -&gt; None:\n    \"\"\"Initializer.\n\n    Args:\n        logits: Raw logits for bin probabilities (before sigmoid), of shape (*batch_shape, num_bins)\n        bound_low: Lower bound of the distribution support, needs to be finite.\n        bound_up: Upper bound of the distribution support, needs to be finite.\n        log_spacing: Whether logarithmic (base = 2) spacing for the bins or linear spacing should be used.\n        validate_args: Whether to validate arguments. Carried over to keep the interface with the base class.\n    \"\"\"\n    self.logits = logits\n    self.bound_low = bound_low\n    self.bound_up = bound_up\n    self.log_spacing = log_spacing\n\n    # Create bin structure (same for all batch dimensions).\n    self.bin_edges, self.bin_centers, self.bin_widths = self._create_bins(\n        num_bins=logits.shape[-1],\n        bound_low=bound_low,\n        bound_up=bound_up,\n        log_spacing=log_spacing,\n        device=logits.device,\n        dtype=logits.dtype,\n    )\n\n    super().__init__(batch_shape=logits.shape[:-1], event_shape=torch.Size([]), validate_args=validate_args)\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the distribution.</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the distribution.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(logits_shape: {self.logits.shape}, bound_low: {self.bound_low}, \"\n        f\"bound_up: {self.bound_up}, log_spacing: {self.log_spacing})\"\n    )\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.cdf","title":"<code>cdf(value)</code>","text":"<p>Compute cumulative distribution function at given values.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>Values at which to compute the CDF. Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>CDF values in [0, 1] corresponding to the input values.</p> <code>Tensor</code> <p>Output shape: same as <code>value</code> shape after broadcasting, i.e., (*sample_shape, *batch_shape).</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def cdf(self, value: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute cumulative distribution function at given values.\n\n    Args:\n        value: Values at which to compute the CDF.\n            Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n    Returns:\n        CDF values in [0, 1] corresponding to the input values.\n        Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n    \"\"\"\n    if self._validate_args:\n        self._validate_sample(value)\n\n    value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n    # Determine number of sample dimensions (dimensions before batch_shape).\n    num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n    # Prepend singleton dimensions for sample_shape to bin_centers.\n    # bin_centers: (*batch_shape, num_bins) -&gt; (*sample_shape, *batch_shape, num_bins)\n    bin_centers_expanded = self.bin_centers.view((1,) * num_sample_dims + self.bin_centers.shape)\n\n    # Prepend singleton dimensions for sample_shape to probs.\n    # probs: (*batch_shape, num_bins) -&gt; (*sample_shape, *batch_shape, num_bins)\n    probs_expanded = self.probs.view((1,) * num_sample_dims + self.probs.shape)\n\n    # Add the bin dimension to the input which is used for comparing with the bin centers.\n    value_expanded = value.unsqueeze(-1)  # shape: (*sample_shape, *batch_shape, 1)\n\n    # Mask for bins with centers &lt;= value.\n    mask = bin_centers_expanded &lt;= value_expanded  # shape: (*sample_shape, *batch_shape, num_bins)\n\n    # Sum the bins for this value by their weighted \"activation\"=probability.\n    return torch.sum(mask * probs_expanded, dim=-1)\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.entropy","title":"<code>entropy()</code>","text":"<p>Compute differential entropy of the distribution.</p> <p>Entropy H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log( p(x) )</p> <p>Note</p> <p>Here, we are doing an approximation by treating each bin as a uniform distribution over its width.</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def entropy(self) -&gt; torch.Tensor:\n    r\"\"\"Compute differential entropy of the distribution.\n\n    Entropy H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log( p(x) )\n\n    Note:\n        Here, we are doing an approximation by treating each bin as a uniform distribution over its width.\n    \"\"\"\n    bin_probs = self.probs\n\n    # Get the PDF values at bin centers.\n    pdf_values = bin_probs / self.bin_widths  # shape: (*batch_shape, num_bins)\n\n    # Entropy \u2248 -\u2211 p_i * log(pdf_i) * bin_width_i.\n    log_pdf = torch.log(pdf_values + 1e-8)  # small epsilon for stability\n    entropy_per_bin = -bin_probs * log_pdf\n\n    # Sum over bins to get total entropy.\n    return torch.sum(entropy_per_bin, dim=-1)\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.expand","title":"<code>expand(batch_shape, _instance=None)</code>","text":"<p>Expand distribution to new batch shape. This creates a new instance.</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def expand(\n    self, batch_shape: torch.Size | list[int] | tuple[int, ...], _instance: Distribution | None = None\n) -&gt; \"BinnedLogitCDF\":\n    \"\"\"Expand distribution to new batch shape. This creates a new instance.\"\"\"\n    expanded_logits = self.logits.expand((*torch.Size(batch_shape), self.num_bins))\n    return BinnedLogitCDF(\n        logits=expanded_logits,\n        bound_low=self.bound_low,\n        bound_up=self.bound_up,\n        log_spacing=self.log_spacing,\n        validate_args=self._validate_args,\n    )\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.icdf","title":"<code>icdf(value)</code>","text":"<p>Compute the inverse CDF, i.e., the quantile function, at the given values.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>Values in [0, 1] at which to compute the inverse CDF. Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Quantiles in [bound_low, bound_up] corresponding to the input CDF values.</p> <code>Tensor</code> <p>Output shape: same as <code>value</code> shape after broadcasting, i.e., (*sample_shape, *batch_shape).</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def icdf(self, value: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the inverse CDF, i.e., the quantile function, at the given values.\n\n    Args:\n        value: Values in [0, 1] at which to compute the inverse CDF.\n            Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n    Returns:\n        Quantiles in [bound_low, bound_up] corresponding to the input CDF values.\n        Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n    \"\"\"\n    if self._validate_args and not (value &gt;= 0).all() and (value &lt;= 1).all():\n        raise ValueError(\"icdf input must be in [0, 1]\")\n\n    value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n    # Compute CDF at bin edges. prepend zeros to the cumsum of probabilities as this is always the first edge.\n    cdf_edges = torch.cat(\n        [\n            torch.zeros(*self.batch_shape, 1, dtype=self.logits.dtype, device=self.logits.device),\n            torch.cumsum(self.probs, dim=-1),  # shape: (*batch_shape, num_bins)\n        ],\n        dim=-1,\n    )  # shape: (*batch_shape, num_bins + 1)\n\n    # Determine number of sample dimensions (dimensions before batch_shape).\n    num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n    # Prepend singleton dimensions for sample_shape to cdf_edges.\n    # cdf_edges: (*batch_shape, num_bins + 1) -&gt; (*sample_shape, *batch_shape, num_bins + 1)\n    cdf_edges = cdf_edges.view((1,) * num_sample_dims + cdf_edges.shape)\n\n    # Prepend singleton dimensions for  both sample_shape and batch_shape.\n    # bin_edges: (num_bins + 1,) -&gt; (*sample_shape, *batch_shape, num_bins + 1)\n    bin_edges_expanded = self.bin_edges.view(\n        (1,) * (num_sample_dims + len(self.batch_shape)) + self.bin_edges.shape\n    )\n\n    # Add bin dimension to value for comparison.\n    value_expanded = value.unsqueeze(-1)\n\n    # Find bins containing the value: left_cdf &lt;= value &lt; right_cdf.\n    bin_mask = (cdf_edges[..., :-1] &lt;= value_expanded) &amp; (value_expanded &lt; cdf_edges[..., 1:])\n    bin_mask = bin_mask.to(self.logits.dtype)\n\n    # Handle edge case where value \u2248 1.0 (use isclose with dtype-appropriate defaults).\n    value_is_one = torch.isclose(value_expanded, torch.ones_like(value_expanded))\n    bin_mask[..., -1] = torch.max(bin_mask[..., -1], value_is_one[..., 0])  # last bin could be selected already\n\n    # Selected the correct bin edges using the mask. Summing is essentially selecting here.\n    # Summing fast and differentiable.\n    cfd_value_bin_starts = torch.sum(bin_mask * cdf_edges[..., :-1], dim=-1)\n    cdf_value_bin_ends = torch.sum(bin_mask * cdf_edges[..., 1:], dim=-1)\n    bin_left_edges = torch.sum(bin_mask * bin_edges_expanded[..., :-1], dim=-1)\n    bin_right_edges = torch.sum(bin_mask * bin_edges_expanded[..., 1:], dim=-1)\n\n    # Avoid division by zero.\n    bin_width = cdf_value_bin_ends - cfd_value_bin_starts\n    safe_bin_width = torch.where(bin_width &gt; 1e-8, bin_width, torch.ones_like(bin_width))\n\n    # Linear interpolation within the bin.\n    alpha = (value - cfd_value_bin_starts) / safe_bin_width\n    quantiles = bin_left_edges + alpha * (bin_right_edges - bin_left_edges)\n\n    return quantiles\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.log_prob","title":"<code>log_prob(value)</code>","text":"<p>Compute log probability density at given values.</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def log_prob(self, value: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute log probability density at given values.\"\"\"\n    return torch.log(self.prob(value) + 1e-8)  # small epsilon for stability\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.prob","title":"<code>prob(value)</code>","text":"<p>Compute probability density at given values.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>Values at which to compute the PDF. Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>PDF values corresponding to the input values.</p> <code>Tensor</code> <p>Output shape: same as <code>value</code> shape after broadcasting, i.e., (*sample_shape, *batch_shape).</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>def prob(self, value: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute probability density at given values.\n\n    Args:\n        value: Values at which to compute the PDF.\n            Expected shape: (*sample_shape, *batch_shape) or broadcastable to it.\n\n    Returns:\n        PDF values corresponding to the input values.\n        Output shape: same as `value` shape after broadcasting, i.e., (*sample_shape, *batch_shape).\n    \"\"\"\n    if self._validate_args:\n        self._validate_sample(value)\n\n    value = value.to(dtype=self.logits.dtype, device=self.logits.device)\n\n    # Determine number of sample dimensions (dimensions before batch_shape).\n    num_sample_dims = len(value.shape) - len(self.batch_shape)\n\n    # Prepend singleton dimensions for sample_shape to bin_edges, bin_widths, and probs.\n    # For all of them, the resulting shape will be: (*sample_shape, *batch_shape, num_bins)\n    bin_edges_left = self.bin_edges[..., :-1]  # shape: (*batch_shape, num_bins)\n    bin_edges_right = self.bin_edges[..., 1:]  # shape: (*batch_shape, num_bins)\n    bin_edges_left = bin_edges_left.view((1,) * num_sample_dims + bin_edges_left.shape)\n    bin_edges_right = bin_edges_right.view((1,) * num_sample_dims + bin_edges_right.shape)\n    bin_widths = self.bin_widths.view((1,) * num_sample_dims + self.bin_widths.shape)\n    probs = self.probs.view((1,) * num_sample_dims + self.probs.shape)\n\n    # Add bin dimension to value for broadcasting.\n    value_expanded = value.unsqueeze(-1)  # shape: (*sample_shape, *batch_shape, 1)\n\n    # Check which bin each value falls into. Result shape: (*sample_shape, *batch_shape, num_bins).\n    in_bin = ((value_expanded &gt;= bin_edges_left) &amp; (value_expanded &lt; bin_edges_right)).to(self.logits.dtype)\n\n    # Handle right edge case (include bound_up in last bin).\n    at_right_edge = torch.isclose(\n        value_expanded, torch.tensor(self.bound_up, dtype=self.logits.dtype, device=self.logits.device)\n    )\n    in_bin[..., -1] = torch.max(in_bin[..., -1], at_right_edge[..., -1])\n\n    # PDF = (probability mass / bin width) for the containing bin.\n    pdf_per_bin = probs / bin_widths  # shape: (*sample_shape, *batch_shape, num_bins)\n\n    # Sum over bins is the same as selecting the bin, as there is only one bin active per value.\n    return torch.sum(in_bin * pdf_per_bin, dim=-1)\n</code></pre>"},{"location":"reference/#binned_cdf.binned_logit_cdf.BinnedLogitCDF.sample","title":"<code>sample(sample_shape=_size)</code>","text":"<p>Sample from the distribution by passing uniformly random draws from [0, 1] thought the inverse CDF.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size | list[int] | tuple[int, ...]</code> <p>Shape of the samples to draw.</p> <code>_size</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples of shape [sample_shape + batch_shape, num_bins].</p> Source code in <code>binned_cdf/binned_logit_cdf.py</code> <pre><code>@torch.no_grad()\ndef sample(self, sample_shape: torch.Size | list[int] | tuple[int, ...] = _size) -&gt; torch.Tensor:\n    \"\"\"Sample from the distribution by passing uniformly random draws from [0, 1] thought the inverse CDF.\n\n    Args:\n        sample_shape: Shape of the samples to draw.\n\n    Returns:\n        Samples of shape [sample_shape + batch_shape, num_bins].\n    \"\"\"\n    shape = torch.Size(sample_shape) + self.batch_shape\n    uniform_samples = torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device)\n    return self.icdf(uniform_samples)\n</code></pre>"},{"location":"development/getting_started/","title":"Getting Started","text":""},{"location":"development/getting_started/#getting-started-for-developers","title":"Getting Started for Developers","text":""},{"location":"development/getting_started/#project-structure","title":"Project Structure","text":"File / Directory Purpose <code>.github</code> CI/CD workflow definitions and a PR template <code>binned_cdf</code> Project import modules <code>docs</code> Documentation directory (better write docs there instead of <code>readme.md</code>) <code>tests</code> Python module unit- &amp; integration tests <code>.pre-commit-config.yaml</code> <code>git</code> hook definitions comsumed by <code>pre-commit</code> <code>license.md</code> The license in its long form <code>mkdocs.yml</code> Documentation config consumed by <code>mkdocs</code> <code>pyproject.toml</code> Project information, dependencies and task runner configurations <code>readme.md</code> General project overview, displayed when visiting GitHub repository <code>uv.lock</code> Contains the locked dependencies to exactly reproduce installations."},{"location":"development/getting_started/#dependency-management-packaging","title":"Dependency Management &amp; Packaging","text":"<p>To keep the dependencies of different projects from interfering with each other, it is highly recommended to create an isolated python environment for every project. We use <code>uv</code> to address this issue. By running <code>uv sync</code> inside the project directory, a new virtual environment is created automatically into which all your dependencies are installed (from the <code>uv.lock</code> file). This is different from running <code>pip install .</code> in an isolated virtual environment as this might use different dependency versions. Afterwards you can run any command within the virtual environment by simply calling</p> <pre><code>uv run &lt;command&gt;\n</code></pre>"},{"location":"development/getting_started/#testing","title":"Testing","text":"<p>Executing</p> <pre><code>uv run pytest --cov\n</code></pre> <p>will run pytest, compute the test coverage and fail if below the minimum coverage defined by the <code>tool.coverage.report.fail_under</code> threshold in the <code>pyproject.toml</code> file.</p>"},{"location":"development/getting_started/#documentation","title":"Documentation","text":"<p>The code documentation is based on <code>mkdocs</code> which converts markdown files into a nicely-rendered web-page. In particular, we use the <code>mkdocs-material</code> package which offers more than just theming. To generate documentation for different versions, <code>mike</code> is used as a plugin within <code>mkdocs</code>.</p> <p>To build and develop docs on a local server, run</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>To deploy the docs to the <code>gh-pages</code> remote branch, call</p> <pre><code>uv run mike deploy --push --update-aliases &lt;version&gt; &lt;alias&gt;\n</code></pre> <p>where <code>&lt;alias&gt;</code> may be any name alias for your version such as <code>latest</code>, <code>stable</code> or <code>whatever</code>.</p> <p>The final documentation is located at:</p> <pre><code>https://famura.github.io/binned-cdf/&lt;alias&gt;\n</code></pre>"},{"location":"development/getting_started/#git-hooks","title":"Git Hooks","text":"<p>We use <code>pre-commit</code> to run git hooks helping you to develop high-quality code. The hooks are configured in the <code>.pre-commit-config.yaml</code> file and executed before commit.</p> <p>For instance, <code>ruff</code> &amp; <code>ruff-format</code> fix the code base in-place to adhere to reasonable coding standards. <code>mypy</code>mypy &amp; <code>ruff</code> lint the code for correctness. These tools are configured via <code>pyproject.toml</code> and <code>.pre-commit-config.yaml</code> files.</p> <p>Installation</p> <p>After you cloned this project and plan to develop in it, don't forget to install these hooks via</p> <pre><code>uv run pre-commit install\n</code></pre> Available pre-commit hooks <pre><code>minimum_pre_commit_version: \"3.6.0\"\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v6.0.0\n    hooks:\n      - id: check-added-large-files\n      - id: check-ast\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: check-shebang-scripts-are-executable\n      - id: check-symlinks\n      - id: check-toml\n      - id: check-yaml\n        args: [\"--unsafe\"]\n      - id: end-of-file-fixer\n      # - id: no-commit-to-branch # default: main, master\n      - id: trailing-whitespace\n\n  - repo: https://github.com/python-jsonschema/check-jsonschema\n    rev: 0.33.3\n    hooks:\n      - id: check-dependabot\n      - id: check-github-actions\n      - id: check-github-workflows\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.13.0\n    hooks:\n      - id: ruff-format\n      - id: ruff\n        args: [\"--unsafe-fixes\"]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.18.1\n    hooks:\n      - id: mypy\n        args: [\"--explicit-package-bases\"]\n</code></pre>"},{"location":"development/getting_started/#github-actions","title":"GitHub Actions","text":"<p>There are basic CI and CD pipelines, executed as GitHub Actions workflow when pushing changes or opening PR's.</p> Available workflows .github/workflows/ci.yaml.github/workflows/cd.yaml <pre><code>name: Continuous Integration\n\non:\n    pull_request:\n        types: [opened, ready_for_review, reopened, synchronize]\n    workflow_call:\n    workflow_dispatch:\n\nconcurrency:\n    group: ${{ github.job }}/${{ github.workflow }}/${{ github.head_ref || github.ref }}\n    cancel-in-progress: true\n\njobs:\n    ci-default-python-version:\n        name: Default Python Version\n        runs-on: ubuntu-latest\n        permissions:\n            contents: write\n            pull-requests: write\n        timeout-minutes: 120\n\n        steps:\n            - name: Check out repository\n              uses: actions/checkout@v5\n\n            - name: Set up uv\n              uses: astral-sh/setup-uv@v7\n\n            - name: Lint &amp; test\n              uses: ./.github/actions/lint-test\n\n            - name: Add coverage comment\n              if: github.event_name == 'pull_request'\n              uses: MishaKav/pytest-coverage-comment@v1\n              with:\n                  coverage-path-prefix: \"binned_cdf/\"\n                  junitxml-path: pytest.xml\n                  pytest-xml-coverage-path: coverage.xml\n\n            - name: Build &amp; deploy temporary docs\n              if: github.event_name == 'pull_request' &amp;&amp; github.event.pull_request.draft == false\n              uses: ./.github/actions/publish-docs\n              with:\n                  alias: pr-${{ github.event.number }}\n                  version: next-pr-${{ github.event.number }}\n\n            - name: Add docs comment\n              if: github.event_name == 'pull_request' &amp;&amp; github.event.pull_request.draft == false\n              uses: marocchino/sticky-pull-request-comment@v2.9.4\n              with:\n                  header: docs-comment\n                  message: |\n                      :books: Created [temporary docs](https://famura.github.io/${{ github.repository }}/pr-${{ github.event.number }}).\n\n                      Useful URLs:\n\n                      - [Coverage](https://famura.github.io/${{ github.repository }}/pr-${{ github.event.number }}/exported/coverage)\n                      - [Tests](https://famura.github.io/${{ github.repository }}/pr-${{ github.event.number }}/exported/tests)\n\n    ci-other-python-versions:\n        if: github.event.pull_request.draft == false\n        name: Other Python Versions\n        runs-on: ubuntu-latest\n        strategy:\n            matrix:\n                python-version: [3.13]\n        timeout-minutes: 120\n\n        steps:\n            - name: Check out repository\n              uses: actions/checkout@v5\n\n            - name: Set up uv\n              uses: astral-sh/setup-uv@v7\n\n            - name: Set Python version to ${{ matrix.python-version }}\n              run: uv python pin ${{ matrix.python-version }}\n\n            - name: Lint &amp; test\n              uses: ./.github/actions/lint-test\n</code></pre> <pre><code>name: Continuous Deployment\n\non:\n    push:\n        branches: [main]\n    workflow_dispatch:\n        inputs:\n            bumped-version-part:\n                description: \"The version part to bump.\"\n                type: choice\n                options:\n                    - major\n                    - minor\n                    - patch\n                default: patch\n                required: true\n\nconcurrency:\n    group: ${{ github.workflow }}/${{ github.head_ref || github.ref }}\n    cancel-in-progress: true\n\njobs:\n    bump-version:\n        name: Bump Version\n        runs-on: ubuntu-latest\n        container: docker:git\n        permissions:\n            contents: write\n        timeout-minutes: 10\n        steps:\n            - name: Install prerequisites\n              run: apk add nodejs\n\n            - name: Check out repository\n              uses: actions/checkout@v5\n\n            - name: Bump version and push tag\n              id: version\n              uses: mathieudutour/github-tag-action@v6.2\n              with:\n                  default_bump: ${{ github.event.inputs.bumped-version-part || 'patch' }}\n                  github_token: ${{ secrets.GITHUB_TOKEN }}\n\n            - name: Add version info\n              run: echo \"Bumped ${VERSION_PART} version part from ${OLD_TAG} to ${NEW_TAG}.\" &gt;&gt; $GITHUB_STEP_SUMMARY\n              env:\n                  VERSION_PART: ${{ steps.version.outputs.release_type }}\n                  OLD_TAG: ${{ steps.version.outputs.previous_tag }}\n                  NEW_TAG: ${{ steps.version.outputs.new_tag }}\n\n    ci:\n        name: CI\n        needs: bump-version\n        uses: ./.github/workflows/ci.yaml\n        secrets: inherit\n        permissions:\n            contents: write\n            pull-requests: write\n\n    deploy:\n        name: Deploy Docs\n        needs: ci\n        if: github.repository == 'famura/binned-cdf'\n        runs-on: ubuntu-latest\n        timeout-minutes: 30\n        permissions:\n            contents: write\n        steps:\n            - name: Check out repository\n              uses: actions/checkout@v5\n              with:\n                  fetch-depth: 0\n\n            - name: Set up uv\n              uses: astral-sh/setup-uv@v7\n\n            - name: Lint &amp; test\n              uses: ./.github/actions/lint-test\n\n            - name: Build &amp; publish docs\n              uses: ./.github/actions/publish-docs\n\n    publish-pypi:\n        name: Publish to PyPI\n        needs: deploy\n        runs-on: ubuntu-latest\n        environment: release\n        timeout-minutes: 15\n        permissions:\n            id-token: write\n        steps:\n            - name: Check out repository\n              uses: actions/checkout@v5\n\n            - name: Set up uv\n              uses: astral-sh/setup-uv@v7\n\n            - name: Build package\n              run: uv build\n\n            - name: Publish to PyPI\n              uses: pypa/gh-action-pypi-publish@release/v1\n\n    update_release_draft:\n        name: Update Release Notes\n        needs: publish-pypi\n        runs-on: ubuntu-latest\n        container: ubuntu:latest\n        timeout-minutes: 10\n        permissions:\n            contents: write\n            pull-requests: read\n        steps:\n            - name: Set GHE_HOST\n              run: echo \"GHE_HOST=${GITHUB_SERVER_URL##https:\\/\\/}\" &gt;&gt; ${GITHUB_ENV}\n\n            - uses: release-drafter/release-drafter@v6\n              env:\n                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre>"},{"location":"development/installation/","title":"Installation","text":""},{"location":"development/installation/#installation-for-developers","title":"Installation for Developers","text":""},{"location":"development/installation/#cloning-the-repository","title":"Cloning the Repository","text":"<p>For development, you need the source code. Clone the repository by executing</p> <pre><code>git clone https://github.com/famura/binned-cdf.git\n</code></pre>"},{"location":"development/installation/#uv","title":"uv","text":"<p>This project is managed by <code>uv</code>, an extremely fast Python package and project manager, written in Rust. This means, however, that <code>uv</code> needs to be installed before you can run the CLI or install the development version. The official <code>uv</code> installation docs show several ways to install []<code>uv</code>]uv on different platforms and under different conditions.</p> Installation for the lazy ones <p>This is just a quick tip on installing uv, better see the official uv docs</p> via curl on macOS &amp; Linux (recommended)via conda (not recommended) <p>Run</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>conda activate &lt;some-environment&gt;\nconda install pip\npip install pipx\npipx install uv\nconda deactivate\n</code></pre>"},{"location":"development/installation/#managing-python","title":"Managing Python","text":"<p>With <code>uv</code>, you can install a suitable python version by running</p> <pre><code>uv python install\n</code></pre> <p>to install the latest stable Python version. See the uv docs for more details.</p>"},{"location":"development/installation/#actual-installation","title":"Actual Installation","text":"<p>The final project installation should be easy. Run this from the projects root level:</p> <pre><code>uv sync\n</code></pre> <p>No project development intended?</p> <p>If you don't need any development setup, you can add the <code>--no-dev</code> flag to skip development dependencies.</p> Computer says no\u2026 <p>In some cases, this does not work right away. Please create and issue such that we can collect failure cases and hints to their solution here.</p> What? Hint placeholder issue placeholder hint"}]}